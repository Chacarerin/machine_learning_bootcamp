# ğŸ“˜ actividad sesiÃ³n 3 --- reducciÃ³n de dimensionalidad con pca (iris)

este proyecto aplica **anÃ¡lisis de componentes principales (pca)** sobre
el dataset clÃ¡sico **iris**. el objetivo es reducir de 4 a 2 dimensiones,
interpretar la varianza explicada y visualizar la proyecciÃ³n en 2d,
evaluando ademÃ¡s el impacto de pca en un clasificador simple (knn).

---

## â–¶ï¸ ejecuciÃ³n rÃ¡pida

```bash
python principal.py
```

- genera todas las salidas en `resultados_sesion3/`.  
- no requiere datasets externos (usa `sklearn.datasets.load_iris`).  

---

## ğŸ“¦ estructura del proyecto

```
actividad_sesion3/
â”œâ”€â”€ principal.py
â”œâ”€â”€ readme.md
â””â”€â”€ resultados_sesion3/
    â”œâ”€â”€ 01_varianza_explicada.png
    â”œâ”€â”€ 02_pca_2d.png
    â”œâ”€â”€ pca_scores_pc12.csv
    â”œâ”€â”€ pca_componentes_pc12.csv
    â”œâ”€â”€ resumen.json
    â”œâ”€â”€ resumen.txt
    â””â”€â”€ metricas_knn.txt
```

---

## 1) dataset y preprocesamiento

- **dataset**: iris (150 observaciones, 4 variables).  
- **preprocesamiento**: escalado con `standardscaler` + reducciÃ³n con
  `pca` a 2 componentes.  

---

## 2) resultados obtenidos

### varianza explicada

- **pc1 = 0.7296**  
- **pc2 = 0.2285**  
- **acumulada pc1+pc2 = 0.9581**  

(ver `01_varianza_explicada.png`)

### proyecciÃ³n 2d

- la visualizaciÃ³n en 2 componentes muestra **una clara separaciÃ³n entre
  las especies de iris**, especialmente setosa, mientras que versicolor
  y virginica presentan cierta superposiciÃ³n.  

(ver `02_pca_2d.png`)

### comparaciÃ³n knn con y sin pca

- **accuracy test sin pca = 0.9211**  
- **accuracy test con pca = 0.8947**  
- **cv(5) promedio sin pca = 0.9600**  
- **cv(5) promedio con pca = 0.9133**  

(ver `metricas_knn.txt`)

---

## 3) anÃ¡lisis

- las **dos primeras componentes principales retienen ~96% de la
  varianza**, lo que permite una representaciÃ³n 2d muy informativa.  
- la separaciÃ³n entre clases mejora en tÃ©rminos de **visualizaciÃ³n** y
  exploraciÃ³n, aunque al usar pca como preprocesamiento en knn se observa
  una **ligera pÃ©rdida de precisiÃ³n** (0.92 â†’ 0.89 en test).  
- pca resulta Ãºtil para **reducir dimensionalidad y ruido** en datasets
  mÃ¡s grandes o con muchas variables, incluso si en iris no mejora
  necesariamente el rendimiento predictivo.  

---

## 4) conclusiÃ³n

- pca en iris confirma que **dos componentes son suficientes** para
  capturar la mayor parte de la variabilidad (95.8%).  
- la **visualizaciÃ³n en 2d** permite interpretar patrones y agrupamientos
  entre especies.  
- como paso de preprocesamiento, **pca puede simplificar el espacio sin
  degradar mucho la performance** de modelos simples como knn.  

---

## ğŸ‘¤ autor

Este proyecto fue desarrollado por **RubÃ©n Schnettler**  
ğŸ“ ViÃ±a del Mar, Chile.  

---

## ğŸ¤– asistencia tÃ©cnica

DocumentaciÃ³n y apoyo en redacciÃ³n por **chatgpt (gpt-5, 2025)**
