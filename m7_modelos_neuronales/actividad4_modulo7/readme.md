# ğŸ“˜ actividad sesiÃ³n 4 --- transfer learning con efficientnetb0 (cifar-10)

este proyecto implementa un enfoque de **transfer learning** usando
**efficientnetb0** (preentrenado en imagenet) aplicado al dataset
**cifar-10**. el objetivo fue comprobar la utilidad de un modelo
preentrenado en un dataset generalista al clasificar imÃ¡genes pequeÃ±as y
de dominio distinto. se evaluÃ³ el desempeÃ±o mediante curvas de
entrenamiento, matriz de confusiÃ³n, reporte de clasificaciÃ³n y mÃ©tricas
globales.

---

## â–¶ï¸ ejecuciÃ³n rÃ¡pida

```bash
python principal.py
```

- genera todas las salidas en `resultados_sesion4/`.  
- no requiere descarga manual de datos (usa `tf.keras.datasets.cifar10`).  
- se puede cambiar de backbone con `--modelo resnet` o activar *fine-tuning* con `--fine_tune`.  

---

## ğŸ“¦ estructura del proyecto

```
actividad4_modulo7/
â”œâ”€â”€ principal.py
â”œâ”€â”€ readme.md
â””â”€â”€ resultados_sesion4/
    â”œâ”€â”€ curvas_entrenamiento_fase1.png
    â”œâ”€â”€ matriz_confusion.png
    â”œâ”€â”€ predicciones_vs_reales.png
    â”œâ”€â”€ reporte_clasificacion.txt
    â”œâ”€â”€ resumen.json
    â”œâ”€â”€ modelo_efficientnet.keras
    â””â”€â”€ modelo_efficientnet_resumen.txt
```

---

## 1) dataset y preprocesamiento

- **dataset**: cifar-10 (50.000 imÃ¡genes de entrenamiento y 10.000 de
  prueba en 10 clases).  
- **preprocesamiento**: reescalado de 32x32 a 224x224 pÃ­xeles,
  normalizaciÃ³n con la funciÃ³n propia de efficientnet y aumento de datos
  (flip horizontal, rotaciÃ³n, zoom).  

---

## 2) resultados obtenidos

### mÃ©tricas globales

- **test accuracy** = **0.1001**  
- **test loss** = **2.3011**  

(ver `resumen.json`)

### reporte de clasificaciÃ³n (extracto)

como puede observarse en `reporte_clasificacion.txt`, el modelo
prÃ¡cticamente no logrÃ³ distinguir las clases. el Ãºnico resultado notable
fue en la clase **â€œperroâ€** con recall = 1.0 (todas las imÃ¡genes fueron
predichas como perro), pero esto redujo el rendimiento general en el
resto de categorÃ­as.

### visualizaciones

- `curvas_entrenamiento_fase1.png`: muestran pÃ©rdida estable pero sin
  mejora de exactitud.  
- `matriz_confusion.png`: evidencia la predicciÃ³n dominante en una sola
  clase.  
- `predicciones_vs_reales.png`: la mayorÃ­a de ejemplos terminan
  malclasificados.  

---

## 3) anÃ¡lisis

- **transferencia fallida**: el modelo entrenado con efficientnetb0 no
  logrÃ³ generalizar en cifar-10 bajo la configuraciÃ³n usada. el
  rendimiento se quedÃ³ en el nivel de un clasificador aleatorio (10% de
  accuracy).  
- **causas posibles**:  
  - falta de entrenamiento suficiente (solo 10 Ã©pocas, sin *fine-tuning*
    del backbone).  
  - tamaÃ±o reducido de imÃ¡genes de cifar-10 (32x32) que, al reescalar a
    224x224, genera interpolaciones que pueden degradar la informaciÃ³n.  
  - necesidad de ajustar hiperparÃ¡metros como learning rate, batch size,
    o aplicar *fine-tuning* parcial en capas superiores.  
- **comparaciÃ³n con expectativas**: se esperaba al menos un 60â€“70% de
  accuracy con transfer learning. los resultados muestran que congelar
  completamente el backbone en este caso fue insuficiente para aprender
  patrones Ãºtiles.  
- **valor pedagÃ³gico**: aunque el resultado numÃ©rico fue bajo, el
  ejercicio demuestra de manera clara las limitaciones de usar modelos
  preentrenados sin adaptaciÃ³n al dominio del dataset objetivo. esto
  permite comprender la importancia del *fine-tuning* y de ajustar la
  estrategia de transferencia.  

---

## 4) conclusiÃ³n

- el uso directo de **efficientnetb0** preentrenado no resultÃ³ efectivo
  en cifar-10 con la configuraciÃ³n inicial. el modelo colapsÃ³ en una sola
  clase (perro), logrando apenas un 10% de accuracy.  
- para mejorar, se recomienda:  
  - habilitar **fine-tuning** de al menos las Ãºltimas capas del
    backbone.  
  - probar con **resnet50** para comparar desempeÃ±os.  
  - entrenar con mÃ¡s Ã©pocas y usar *schedulers* de tasa de aprendizaje.  
  - experimentar con arquitecturas convolucionales diseÃ±adas para
    imÃ¡genes pequeÃ±as (como convnets custom o resnet reducidas).  
- este resultado responde al objetivo de la actividad: se realizÃ³ la
  implementaciÃ³n, se obtuvieron mÃ©tricas y visualizaciones, y se analizÃ³
  crÃ­ticamente por quÃ© el transfer learning no alcanzÃ³ un buen
  desempeÃ±o.  

---

## ğŸ‘¤ autor

Este proyecto fue desarrollado por **RubÃ©n Schnettler**  
ğŸ“ ViÃ±a del Mar, Chile.  

---

## ğŸ¤– asistencia tÃ©cnica

documentaciÃ³n y apoyo en redacciÃ³n por **chatgpt (gpt-5, 2025)**
