# combinacion: loss=categorical_crossentropy, opt=adam
test_accuracy: 0.8635
test_loss: 0.3833

classification_report:
              precision    recall  f1-score   support

           0     0.8452    0.7860    0.8145      1000
           1     0.9815    0.9530    0.9670      1000
           2     0.7706    0.7660    0.7683      1000
           3     0.7986    0.9160    0.8533      1000
           4     0.7266    0.8160    0.7687      1000
           5     0.9624    0.9460    0.9541      1000
           6     0.7184    0.5920    0.6491      1000
           7     0.9100    0.9600    0.9343      1000
           8     0.9613    0.9700    0.9657      1000
           9     0.9647    0.9300    0.9470      1000

    accuracy                         0.8635     10000
   macro avg     0.8639    0.8635    0.8622     10000
weighted avg     0.8639    0.8635    0.8622     10000

confusion_matrix:
[[786   5  13  74   5   0 105   0  12   0]
 [  2 953   1  34   6   0   2   0   2   0]
 [  9   1 766  16 145   1  59   0   3   0]
 [ 14   8   8 916  30   0  22   0   2   0]
 [  0   0 100  44 816   0  38   0   2   0]
 [  0   0   0   1   0 946   0  31   2  20]
 [118   3 101  56 114   0 592   0  16   0]
 [  0   0   0   0   0  26   0 960   0  14]
 [  1   1   5   5   7   2   5   4 970   0]
 [  0   0   0   1   0   8   1  60   0 930]]