# ğŸ“˜ actividad sesiÃ³n 2 --- rnn (lstm) y red generativa adversaria (gan)

este proyecto implementa dos enfoques complementarios de aprendizaje profundo:  
1. una **red recurrente (lstm)** para la **clasificaciÃ³n de sentimientos** en el dataset **imdb**.  
2. una **red generativa adversaria (gan)** para la **generaciÃ³n de imÃ¡genes sintÃ©ticas** del dataset **mnist**.

---

## â–¶ï¸ ejecuciÃ³n rÃ¡pida

```bash
python principal.py
```

- genera todas las salidas en `resultados_sesion2/`.  
- no requiere descarga manual de datos (usa `tf.keras.datasets.imdb` y `tf.keras.datasets.mnist`).  

---

## ğŸ“¦ estructura del proyecto

```
actividad2_modulo7/
â”œâ”€â”€ principal.py
â”œâ”€â”€ readme.md
â””â”€â”€ resultados_sesion2/
    â”œâ”€â”€ imdb_curvas.png
    â”œâ”€â”€ imdb_matriz_confusion.png
    â”œâ”€â”€ imdb_resumen.txt
    â”œâ”€â”€ gan_curvas.png
    â”œâ”€â”€ gan_resumen.txt
    â”œâ”€â”€ resumen.txt
    â””â”€â”€ gan_iter_*.png
```

---

## 1) dataset y preprocesamiento

### imdb
- **dataset**: imdb (50.000 reseÃ±as de pelÃ­culas, etiquetadas como positivas o negativas).  
- **preprocesamiento**: tokenizaciÃ³n y padding de secuencias de texto.  

### mnist
- **dataset**: mnist (60.000 imÃ¡genes de entrenamiento, 10.000 de prueba).  
- **preprocesamiento**: normalizaciÃ³n de pÃ­xeles a rango [0,1].  

---

## 2) resultados obtenidos

### parte a â€“ imdb con lstm âœ… (mejor desempeÃ±o en clasificaciÃ³n)

- **test accuracy = 0.7580**  
- **test loss = 0.5547**  
- **matriz de confusiÃ³n**: tn=9742, fp=2758, fn=3291, tp=9209  

(ver `imdb_curvas.png` y `imdb_matriz_confusion.png`)

### parte b â€“ gan para mnist

- **iteraciones = 3000**  
- **batch size = 128**  
- el generador logra producir dÃ­gitos sintÃ©ticos reconocibles tras el entrenamiento.  

(ver `gan_curvas.png` y `gan_iter_*.png`)

---

## 3) anÃ¡lisis

- la **lstm aplicada a imdb** logra un desempeÃ±o aceptable en clasificaciÃ³n de sentimientos (â‰ˆ76% de accuracy). el anÃ¡lisis de la matriz de confusiÃ³n muestra un equilibrio razonable entre verdaderos positivos y negativos, aunque con espacio de mejora en los falsos negativos.  
- la **gan en mnist** evidencia la capacidad de confrontaciÃ³n entre generador y discriminador, mejorando progresivamente la calidad visual de los dÃ­gitos. con 3000 iteraciones, ya se obtienen resultados claros y estables.  

---

## 4) conclusiÃ³n

- **clasificaciÃ³n**: la lstm se consolida como una arquitectura efectiva para tareas de procesamiento de lenguaje natural, aunque con margen de mejora en optimizaciÃ³n de hiperparÃ¡metros.  
- **generaciÃ³n**: la gan demuestra el poder de los modelos generativos para crear ejemplos sintÃ©ticos, siendo un buen punto de partida para futuras aplicaciones en dominios mÃ¡s complejos.  

---

## ğŸ‘¤ autor

este proyecto fue desarrollado por **RubÃ©n Schnettler**  
ğŸ“ ViÃ±a del Mar, Chile.  

---

## ğŸ¤– asistencia tÃ©cnica

documentaciÃ³n y apoyo en redacciÃ³n por **chatgpt (gpt-5, 2025)**
