#  Comparaci贸n de M茅todos de Boosting y Bagging en Predicci贸n de Ingresos

Este proyecto aplica t茅cnicas de ensamblado (ensemble methods) como Bagging y Boosting para predecir si una persona gana m谩s de $50.000 USD anuales, utilizando el dataset Adult Income.

##  Caracter铆sticas

- Dataset: Adult Income (OpenML)
- Predicci贸n binaria (ingreso >50K)
- Modelos implementados:
  - Random Forest (Bagging)
  - AdaBoost (Boosting)
  - XGBoost (Boosting)
- Evaluaci贸n con m茅tricas cl谩sicas:
  - Accuracy
  - Matriz de Confusi贸n
- Comparaci贸n visual del rendimiento

##  Estructura del Proyecto

```
actividad3_modulo5/
 principal.py             # C贸digo completo del proyecto
 readme.md                # Este archivo
 requirements.txt         # Paquetes utilizados
 captura_terminal.txt     # Registro de ejecuci贸n
 Figure_RandomForest.png  # Matriz de confusi贸n RandomForest
 Figure_AdaBoost.png      # Matriz de confusi贸n AdaBoost
 Figure_XGBoost.png       # Matriz de confusi贸n XGBoost
 Figure_Accuracy.png      # Comparaci贸n de accuracy
```

## 锔 Uso del Proyecto

1. Instalar dependencias:
```bash
pip install -r requirements.txt
```

2. Ejecutar el script:
```bash
python principal.py
```

El script entrena los modelos, eval煤a su rendimiento, genera gr谩ficos y muestra el tiempo total de ejecuci贸n.

##  M茅tricas utilizadas

- Accuracy (Exactitud)
- Matriz de Confusi贸n
- Comparaci贸n visual de rendimiento (barra de accuracy)

##  Dataset

El dataset utilizado es el **Adult Income Dataset**, que contiene registros sobre caracter铆sticas demogr谩ficas y ocupacionales. El objetivo es predecir si una persona gana m谩s de $50.000 USD anuales. Dataset cargado desde OpenML.

Fuente:  
https://www.openml.org/d/1590

##  An谩lisis final y comparaci贸n cr铆tica

**驴Qu茅 t茅cnica tuvo mejor desempe帽o?**  
XGBoost obtuvo el mejor rendimiento en t茅rminos de accuracy, seguido de Random Forest y luego AdaBoost. En las matrices de confusi贸n se observa que XGBoost tiene un buen balance entre verdaderos positivos y negativos.

**驴Qu茅 ventajas o limitaciones present贸 cada enfoque?**  
- **Random Forest** es robusto y f谩cil de ajustar, pero puede sobreajustar si no se regula bien.
- **AdaBoost** es sensible al ruido y errores mal clasificados.
- **XGBoost** es el m谩s preciso, eficiente y escalable, aunque m谩s complejo de configurar.

**驴Qu茅 modelo recomendar铆as para producci贸n?**  
Recomiendo **XGBoost**, ya que mostr贸 mayor exactitud y buena generalizaci贸n. Adem谩s, tiene opciones para interpretaci贸n como importancia de variables y tolera desequilibrios de clases.

## М Interpretaci贸n de la matriz de confusi贸n

Cada matriz tiene la siguiente estructura:

```
              Predicho
            0        1
Real  0   [TN]     [FP]
      1   [FN]     [TP]
```

- **TN (Verdaderos Negativos)**: correctamente predichos como <=50K  
- **TP (Verdaderos Positivos)**: correctamente predichos como >50K  
- **FP**: clasificados como >50K pero en realidad no lo eran  
- **FN**: clasificados como <=50K pero en realidad s铆 ganaban m谩s  

El ideal es tener la mayor cantidad posible en la diagonal (TN y TP).

##  Autor

Este proyecto fue desarrollado por Rub茅n Schnettler.  
Vi帽a del Mar, Chile.

##  Asistencia T茅cnica

Apoyo en depuraci贸n de c贸digo y documentaci贸n por:  
ChatGPT (gpt-4o, build 2025-07).
